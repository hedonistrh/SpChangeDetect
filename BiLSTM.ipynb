{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we are trying to reproduce [the paper](https://pdfs.semanticscholar.org/edff/b62b32ffcc2b5cc846e26375cb300fac9ecc.pdf) for speaker change detection\n",
    "\n",
    "## Review\n",
    "\n",
    "**Sequence Labelling** \n",
    "\n",
    "They think this task as a binary classification. Thus, they label changing frame as a **1** and non-changing frame as a **0**. So that, they use the _binary cross-entropy loss function_.\n",
    "\n",
    "**Network Architecture**\n",
    "- 2 Bi-LSTM\n",
    "    - 64 and 32 outputs respectively.\n",
    "- Multi Layer Perceptron\n",
    "    - 3 Fully Connected Feedforward Layers\n",
    "        - 40, 20, 1 dimensional respectively.\n",
    "    - Tanh activation for first 2 layer\n",
    "    - Sigmoid activation for last layer\n",
    "    \n",
    "**Feature Extraction**\n",
    "- \"35-dimensional acoustic features are extracted every 16ms on a 32ms window using [Yaafe toolkit](http://yaafe.sourceforge.net).\"\n",
    "    - 11 Mel-Frequency Cepstral Coefficients (MFCC), \n",
    "    - Their first and second derivatives,\n",
    "    - First and second derivatives of the energy.\n",
    "\n",
    "**Class Imbalance**\n",
    "\n",
    "- _\"The number of positive labels isincreased artificially by labeling as positive every frame in the direct neighborhood of the manually annotated change point.\"_\n",
    "- A positive neighborhood of 100ms (50ms on both sides) is used around each change point, to partially solve the class imbalance problem.\n",
    "\n",
    "**Subsequences**\n",
    "\n",
    "- _\"The long audio sequences are split into short fixed-length overlapping sequences.\"_\n",
    "\n",
    "**Prediction**\n",
    "\n",
    "- _\"Finally, local score maxima exceeding a pre-determined threshold Î¸ are marked as speaker change points.\"_\n",
    "\n",
    "**Training**\n",
    "\n",
    "- Subsequences for training are 3.2s long with a step of 800ms (i.e. two adjacent sequences overlap by 75%)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code\n",
    "\n",
    "### Feature Extraction\n",
    "\n",
    "We will use Yaafe Toolkit. (To see all available features, you can use _!yaafe -l_) To learn how we can do that, start with http://yaafe.github.io/Yaafe/manual/quickstart.html#quick-start-using-yaafe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can view a description of each feature (or output format) with the -d option:\n",
    "\n",
    "!yaafe -d MFCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yaafe -d Energy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's determine blockSize and stepSize. \n",
    "\n",
    "If we have 16kHz audio signal(in AMI, we have 16kHz), for 32 ms block, we need 16x32, For the stepsize as 16 ms, we need 16x16 size.\n",
    "\n",
    "We need these features:\n",
    "\n",
    "- mfcc: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11\n",
    "- mfcc_d1: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 > Derivate DOrder=1\n",
    "- mfcc_d2: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 > Derivate DOrder=2\n",
    "- energy_d1: Energy blockSize=512 stepSize=256  > Derivate DOrder=1\n",
    "- energy_d2: Energy blockSize=512 stepSize=256  > Derivate DOrder=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract all of these, we will use [this technique](http://yaafe.github.io/Yaafe/manual/quickstart.html#extract-several-features). Shortly, we will write all these features into single text file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"featureplan.txt\", \"w\")\n",
    "f.write(\"mfcc: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 \\n\"\n",
    "        \"mfcc_d1: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 > Derivate DOrder=1 \\n\"\n",
    "        \"mfcc_d2: MFCC blockSize=512 stepSize=256 CepsNbCoeffs=11 > Derivate DOrder=2 \\n\"\n",
    "        \"energy_d1: Energy blockSize=512 stepSize=256  > Derivate DOrder=1 \\n\"\n",
    "        \"energy_d2: Energy blockSize=512 stepSize=256  > Derivate DOrder=2\")\n",
    "f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat featureplan.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!yaafe -c featureplan.txt -r 16000 a2002011001-e02-16kHz.wav -p Precision=8 -p Metadata=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With Librosa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "def wav_to_matrix(filename, hop, win_len): # hop and win_len in milisecond \n",
    "    audio, sr = librosa.load(filename)\n",
    "    # https://github.com/librosa/librosa/issues/584\n",
    "    mfccs = librosa.feature.mfcc(audio, sr, n_mfcc=11, hop_length=int(float(hop/1000)*sr), n_fft=int(float(win_len/1000)*sr))\n",
    "    mfccs_d1 = librosa.feature.delta(mfccs)\n",
    "    mfccs_d2 = librosa.feature.delta(mfccs, order=2)\n",
    "    energy = librosa.feature.rmse(y=audio, hop_length=int(float(hop/1000)*sr), frame_length=int(float(win_len/1000)*sr))\n",
    "    energy_d1 = librosa.feature.delta(energy)\n",
    "    energy_d2 = librosa.feature.delta(energy, order=2)\n",
    "    print (mfccs.shape)\n",
    "    print (mfccs_d1.shape)\n",
    "    print (mfccs_d2.shape)\n",
    "    print (energy_d1.shape)\n",
    "    print (energy_d2.shape)\n",
    "\n",
    "    a = np.vstack((mfccs, mfccs_d1, mfccs_d2, energy_d1, energy_d2))\n",
    "    # line_mfccs = np.ravel(mfccs, order='F')\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib.pyplot as pp\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def create_data_for_supervised(root_dir, hop, win_len, from_ep = 0, to_ep=0, boost_for_imbalance=False):\n",
    "    all_audio_paths = glob.glob(os.path.join(root_dir, '*wav'))\n",
    "    matrix_of_all_audio = []\n",
    "    \n",
    "    output_array = []\n",
    "    \n",
    "    num = 0\n",
    "    \n",
    "    for single_audio_path in all_audio_paths:\n",
    "        num += 1\n",
    "        \n",
    "        if ((num >= from_ep) and (num < to_ep)):\n",
    "            \n",
    "            end_time_array_second = []\n",
    "\n",
    "            filename = (single_audio_path.split(\"/\")[-1]).split(\".\")[0]\n",
    "            matrix_of_single_audio = wav_to_matrix(single_audio_path, hop, win_len)\n",
    "            array_of_single_audio = np.ravel(matrix_of_single_audio)\n",
    "\n",
    "            if (matrix_of_single_audio is not None):\n",
    "\n",
    "                print (matrix_of_single_audio.shape)\n",
    "                matrix_of_all_audio.extend(array_of_single_audio)\n",
    "                print (single_audio_path + \" is done.\")\n",
    "\n",
    "                main_set = \"./txt_files/\" + filename + \"_end_time.txt\"# FILENAME PATH for TXT\n",
    "\n",
    "                with open(main_set) as f:\n",
    "                    content = f.readlines()\n",
    "\n",
    "                # you may also want to remove whitespace characters like `\\n` at the end of each line\n",
    "\n",
    "\n",
    "                # need to open text file\n",
    "                # after that, point the end point of speaker\n",
    "                # add 1 to point of speaker, add 0 otherwise\n",
    "                # time is in second format at the txt file\n",
    "                content = [x.strip() for x in content] \n",
    "\n",
    "                for single_line in content:\n",
    "\n",
    "                    end_time_array_second.append(single_line)\n",
    "\n",
    "                    # we use following method to get milisecond version\n",
    "                    # float(win_len + ((offset+100) * hop)) \n",
    "                    # we need to inversion of that\n",
    "\n",
    "                output_array = np.zeros(matrix_of_single_audio.shape[1])\n",
    "\n",
    "                for end_time in end_time_array_second:\n",
    "                    end_time_ms = float(end_time)*1000\n",
    "                    which_start_hop = (end_time_ms-win_len)/hop # now we know, milisecond version of change\n",
    "                                                # which is located after which_hop paramater\n",
    "                                                # add 2 and round to up\n",
    "                    which_end_hop = end_time_ms/hop # round to up\n",
    "                    \n",
    "                    start_location = math.ceil(which_start_hop + 1)\n",
    "                    end_location = math.ceil(which_end_hop)\n",
    "\n",
    "                    # print (\"s:\", start_location)\n",
    "                    # print (\"e:\", end_location)\n",
    "                    if (boost_for_imbalance==False):\n",
    "                        output_array[start_location:end_location+1] = 1.0\n",
    "                        \n",
    "                    else:\n",
    "                        output_array[start_location-2:end_location+3] = 1.0\n",
    "\n",
    "\n",
    "            # print (output_array)\n",
    "            print (output_array.mean())\n",
    "            # ar = np.arange(matrix_of_single_audio.shape[1]) # just as an example array\n",
    "            # pp.plot(ar, output_array, 'x')\n",
    "            # pp.show()\n",
    "                \n",
    "            \n",
    "    audio_array = np.asarray(matrix_of_all_audio)\n",
    "    audio_array = np.reshape(matrix_of_all_audio, (35,-1))\n",
    "   \n",
    "        \n",
    "    input_array = np.asarray(audio_array)\n",
    "    input_array = input_array.reshape((len(input_array), np.prod(input_array.shape[1:])))  \n",
    "    print(input_array.shape)\n",
    "    \n",
    "    output_array = np.asarray(output_array)\n",
    "    output_array = np.expand_dims(output_array, axis=0)\n",
    "    print(output_array.shape)\n",
    "\n",
    "    return (input_array, output_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 154884)\n",
      "(11, 154884)\n",
      "(11, 154884)\n",
      "(1, 154884)\n",
      "(1, 154884)\n",
      "(35, 154884)\n",
      "./amicorpus/IN1002/audio/IN1002.Mix-Headset.wav is done.\n",
      "0.025238242814\n",
      "(35, 154884)\n",
      "(1, 154884)\n"
     ]
    }
   ],
   "source": [
    "inn, out = create_data_for_supervised (\"./amicorpus/*/audio/\", 16, 32, 0, 2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = wav_to_matrix(\"How to Read a Research Paper.mp3\", 16, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = wav_to_matrix(\"How to Read a Research Paper.mp3\", 32, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as pp\n",
    "%matplotlib inline\n",
    "\n",
    "pp.plot(np.swapaxes(k, 0, 1))\n",
    "pp.axhline(y=0.5, color='r', linestyle='-')\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Subsequences with Label\n",
    "\n",
    "At that point, we should create training and test data with their label. Also, we can use directly [pyannote.metrics](https://github.com/pyannote/pyannote-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning Architecture\n",
    "\n",
    "We can directly upload the model's architecture from the .yml file which is provided by writer.\n",
    "\n",
    "However, I want to directly write all steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author's .yml files\n",
    "\n",
    "!wget https://raw.githubusercontent.com/yinruiqing/change_detection/master/model/architecture.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/engine/topology.py:1271: UserWarning: Update your `InputLayer` call to the Keras 2 API: `InputLayer(batch_input_shape=(None, 320..., name=\"labeling_input\", sparse=False, dtype=\"float32\")`\n",
      "  return cls(**config)\n",
      "/home/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/layers/recurrent.py:2209: UserWarning: The `input_dim` and `input_length` arguments in recurrent layers are deprecated. Use `input_shape` instead.\n",
      "  return cls(**config)\n",
      "/home/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/layers/recurrent.py:2209: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"tanh\", batch_input_shape=(None, 320..., go_backwards=False, input_dtype=\"float32\", name=\"forward_lstm_0\", return_sequences=True, stateful=False, trainable=True, unroll=False, unit_forget_bias=True, input_shape=(None, 35), units=32, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0.0, recurrent_dropout=0.0, implementation=0)`\n",
      "  return cls(**config)\n",
      "/home/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/layers/recurrent.py:2068: UserWarning: `implementation=0` has been deprecated, and now defaults to `implementation=1`.Please update your layer call.\n",
      "  warnings.warn('`implementation=0` has been deprecated, '\n",
      "/home/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/layers/recurrent.py:2209: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(activation=\"tanh\", go_backwards=False, name=\"forward_lstm_1\", return_sequences=True, stateful=False, trainable=True, unroll=False, unit_forget_bias=True, input_shape=(None, 64), units=20, kernel_initializer=\"glorot_uniform\", recurrent_initializer=\"orthogonal\", recurrent_activation=\"hard_sigmoid\", kernel_regularizer=None, bias_regularizer=None, recurrent_regularizer=None, dropout=0.0, recurrent_dropout=0.0, implementation=0)`\n",
      "  return cls(**config)\n",
      "/home/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/engine/topology.py:1271: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", activity_regularizer=None, input_dim=40, name=\"mlp_0\", trainable=True, units=40, kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/engine/topology.py:1271: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"tanh\", activity_regularizer=None, input_dim=40, name=\"mlp_1\", trainable=True, units=10, kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n",
      "/home/herdogan/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/engine/topology.py:1271: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"sigmoid\", activity_regularizer=None, input_dim=10, name=\"labeling_output\", trainable=True, units=1, kernel_initializer=\"glorot_uniform\", kernel_regularizer=None, bias_regularizer=None, kernel_constraint=None, bias_constraint=None, use_bias=True)`\n",
      "  return cls(**config)\n"
     ]
    }
   ],
   "source": [
    "# Load to model\n",
    "\n",
    "from keras.models import model_from_yaml\n",
    "yaml_file = open('architecture.yml', 'r')\n",
    "loaded_model_yaml = yaml_file.read()\n",
    "yaml_file.close()\n",
    "model = model_from_yaml(loaded_model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "print (keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "labeling_input (InputLayer)  (None, 320, 35)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 320, 64)           17408     \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 320, 40)           13600     \n",
      "_________________________________________________________________\n",
      "timedistributed_1 (TimeDistr (None, 320, 40)           1640      \n",
      "_________________________________________________________________\n",
      "timedistributed_2 (TimeDistr (None, 320, 10)           410       \n",
      "_________________________________________________________________\n",
      "timedistributed_3 (TimeDistr (None, 320, 1)            11        \n",
      "=================================================================\n",
      "Total params: 33,069\n",
      "Trainable params: 33,069\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import optimizers\n",
    "import keras\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "from keras.layers.advanced_activations import *\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "\n",
    "\n",
    "frame_shape = (320, 35)\n",
    "\n",
    "## Network Architecture\n",
    "\n",
    "input_frame = keras.Input(frame_shape, name='main_input')\n",
    "\n",
    "bidirectional_1 = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(input_frame)\n",
    "bidirectional_2 = layers.Bidirectional(layers.LSTM(20, activation='tanh', return_sequences=True))(bidirectional_1)\n",
    "\n",
    "tdistributed_1 = layers.TimeDistributed(layers.Dense(40, activation='tanh'))(bidirectional_2)\n",
    "tdistributed_2 = layers.TimeDistributed(layers.Dense(10, activation='tanh'))(tdistributed_1)\n",
    "tdistributed_3 = layers.TimeDistributed(layers.Dense(1, activation='sigmoid'))(tdistributed_2)\n",
    "\n",
    "## Source: https://stackoverflow.com/questions/37743574/hard-limiting-threshold-activation-function-in-tensorflow\n",
    "def step_activation(x):\n",
    "    threshold = 0.4\n",
    "    cond = tf.less(x, tf.fill(value=threshold, dims=tf.shape(x)))\n",
    "    out = tf.where(cond, tf.zeros(tf.shape(x)), tf.ones(tf.shape(x)))\n",
    "\n",
    "    return out\n",
    "\n",
    "step_activation = layers.Dense(1, activation=step_activation, name='threshold_activation')(tdistributed_3)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(input_frame, step_activation)\n",
    "\n",
    "rmsprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=\"rmsprop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "main_input (InputLayer)      (None, 320, 35)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_17 (Bidirectio (None, 320, 64)           17408     \n",
      "_________________________________________________________________\n",
      "bidirectional_18 (Bidirectio (None, 320, 40)           13600     \n",
      "_________________________________________________________________\n",
      "time_distributed_25 (TimeDis (None, 320, 40)           1640      \n",
      "_________________________________________________________________\n",
      "time_distributed_26 (TimeDis (None, 320, 10)           410       \n",
      "_________________________________________________________________\n",
      "time_distributed_27 (TimeDis (None, 320, 1)            11        \n",
      "_________________________________________________________________\n",
      "threshold_activation (Dense) (None, 320, 1)            2         \n",
      "=================================================================\n",
      "Total params: 33,071\n",
      "Trainable params: 33,071\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save our model\n",
    "\n",
    "model_yaml = model.to_yaml()\n",
    "with open(\"model.yaml\", \"w\") as yaml_file:\n",
    "    yaml_file.write(model_yaml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backend: tensorflow\n",
      "class_name: Model\n",
      "config:\n",
      "  input_layers:\n",
      "  - [main_input, 0, 0]\n",
      "  layers:\n",
      "  - class_name: InputLayer\n",
      "    config:\n",
      "      batch_input_shape: !!python/tuple [null, 320, 35]\n",
      "      dtype: float32\n",
      "      name: main_input\n",
      "      sparse: false\n",
      "    inbound_nodes: []\n",
      "    name: main_input\n",
      "  - class_name: Bidirectional\n",
      "    config:\n",
      "      layer:\n",
      "        class_name: LSTM\n",
      "        config:\n",
      "          activation: tanh\n",
      "          activity_regularizer: null\n",
      "          bias_constraint: null\n",
      "          bias_initializer:\n",
      "            class_name: Zeros\n",
      "            config: {}\n",
      "          bias_regularizer: null\n",
      "          dropout: 0.0\n",
      "          go_backwards: false\n",
      "          implementation: 1\n",
      "          kernel_constraint: null\n",
      "          kernel_initializer:\n",
      "            class_name: VarianceScaling\n",
      "            config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "          kernel_regularizer: null\n",
      "          name: lstm_7\n",
      "          recurrent_activation: hard_sigmoid\n",
      "          recurrent_constraint: null\n",
      "          recurrent_dropout: 0.0\n",
      "          recurrent_initializer:\n",
      "            class_name: Orthogonal\n",
      "            config: {gain: 1.0, seed: null}\n",
      "          recurrent_regularizer: null\n",
      "          return_sequences: true\n",
      "          return_state: false\n",
      "          stateful: false\n",
      "          trainable: true\n",
      "          unit_forget_bias: true\n",
      "          units: 64\n",
      "          unroll: false\n",
      "          use_bias: true\n",
      "      merge_mode: concat\n",
      "      name: bidirectional_7\n",
      "      trainable: true\n",
      "    inbound_nodes:\n",
      "    - - - main_input\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: bidirectional_7\n",
      "  - class_name: Bidirectional\n",
      "    config:\n",
      "      layer:\n",
      "        class_name: LSTM\n",
      "        config:\n",
      "          activation: tanh\n",
      "          activity_regularizer: null\n",
      "          bias_constraint: null\n",
      "          bias_initializer:\n",
      "            class_name: Zeros\n",
      "            config: {}\n",
      "          bias_regularizer: null\n",
      "          dropout: 0.0\n",
      "          go_backwards: false\n",
      "          implementation: 1\n",
      "          kernel_constraint: null\n",
      "          kernel_initializer:\n",
      "            class_name: VarianceScaling\n",
      "            config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "          kernel_regularizer: null\n",
      "          name: lstm_8\n",
      "          recurrent_activation: hard_sigmoid\n",
      "          recurrent_constraint: null\n",
      "          recurrent_dropout: 0.0\n",
      "          recurrent_initializer:\n",
      "            class_name: Orthogonal\n",
      "            config: {gain: 1.0, seed: null}\n",
      "          recurrent_regularizer: null\n",
      "          return_sequences: true\n",
      "          return_state: false\n",
      "          stateful: false\n",
      "          trainable: true\n",
      "          unit_forget_bias: true\n",
      "          units: 40\n",
      "          unroll: false\n",
      "          use_bias: true\n",
      "      merge_mode: concat\n",
      "      name: bidirectional_8\n",
      "      trainable: true\n",
      "    inbound_nodes:\n",
      "    - - - bidirectional_7\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: bidirectional_8\n",
      "  - class_name: TimeDistributed\n",
      "    config:\n",
      "      layer:\n",
      "        class_name: Dense\n",
      "        config:\n",
      "          activation: tanh\n",
      "          activity_regularizer: null\n",
      "          bias_constraint: null\n",
      "          bias_initializer:\n",
      "            class_name: Zeros\n",
      "            config: {}\n",
      "          bias_regularizer: null\n",
      "          kernel_constraint: null\n",
      "          kernel_initializer:\n",
      "            class_name: VarianceScaling\n",
      "            config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "          kernel_regularizer: null\n",
      "          name: dense_10\n",
      "          trainable: true\n",
      "          units: 40\n",
      "          use_bias: true\n",
      "      name: time_distributed_10\n",
      "      trainable: true\n",
      "    inbound_nodes:\n",
      "    - - - bidirectional_8\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: time_distributed_10\n",
      "  - class_name: TimeDistributed\n",
      "    config:\n",
      "      layer:\n",
      "        class_name: Dense\n",
      "        config:\n",
      "          activation: tanh\n",
      "          activity_regularizer: null\n",
      "          bias_constraint: null\n",
      "          bias_initializer:\n",
      "            class_name: Zeros\n",
      "            config: {}\n",
      "          bias_regularizer: null\n",
      "          kernel_constraint: null\n",
      "          kernel_initializer:\n",
      "            class_name: VarianceScaling\n",
      "            config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "          kernel_regularizer: null\n",
      "          name: dense_11\n",
      "          trainable: true\n",
      "          units: 20\n",
      "          use_bias: true\n",
      "      name: time_distributed_11\n",
      "      trainable: true\n",
      "    inbound_nodes:\n",
      "    - - - time_distributed_10\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: time_distributed_11\n",
      "  - class_name: TimeDistributed\n",
      "    config:\n",
      "      layer:\n",
      "        class_name: Dense\n",
      "        config:\n",
      "          activation: sigmoid\n",
      "          activity_regularizer: null\n",
      "          bias_constraint: null\n",
      "          bias_initializer:\n",
      "            class_name: Zeros\n",
      "            config: {}\n",
      "          bias_regularizer: null\n",
      "          kernel_constraint: null\n",
      "          kernel_initializer:\n",
      "            class_name: VarianceScaling\n",
      "            config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "          kernel_regularizer: null\n",
      "          name: dense_12\n",
      "          trainable: true\n",
      "          units: 1\n",
      "          use_bias: true\n",
      "      name: time_distributed_12\n",
      "      trainable: true\n",
      "    inbound_nodes:\n",
      "    - - - time_distributed_11\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: time_distributed_12\n",
      "  - class_name: Dense\n",
      "    config:\n",
      "      activation: step_activation\n",
      "      activity_regularizer: null\n",
      "      bias_constraint: null\n",
      "      bias_initializer:\n",
      "        class_name: Zeros\n",
      "        config: {}\n",
      "      bias_regularizer: null\n",
      "      kernel_constraint: null\n",
      "      kernel_initializer:\n",
      "        class_name: VarianceScaling\n",
      "        config: {distribution: uniform, mode: fan_avg, scale: 1.0, seed: null}\n",
      "      kernel_regularizer: null\n",
      "      name: threshold_activation\n",
      "      trainable: true\n",
      "      units: 1\n",
      "      use_bias: true\n",
      "    inbound_nodes:\n",
      "    - - - time_distributed_12\n",
      "        - 0\n",
      "        - 0\n",
      "        - {}\n",
      "    name: threshold_activation\n",
      "  name: model_4\n",
      "  output_layers:\n",
      "  - [threshold_activation, 0, 0]\n",
      "keras_version: 2.1.6\n"
     ]
    }
   ],
   "source": [
    "# To look our model\n",
    "\n",
    "!cat model.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 154884)\n",
      "(11, 154884)\n",
      "(11, 154884)\n",
      "(1, 154884)\n",
      "(1, 154884)\n",
      "(35, 154884)\n",
      "./amicorpus/IN1002/audio/IN1002.Mix-Headset.wav is done.\n",
      "0.025238242814\n",
      "(35, 154884)\n",
      "(1, 154884)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected threshold_activation to have 3 dimensions, but got array with shape (645, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9667c147a737>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m                \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m                \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                shuffle=True)\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;31m# https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1628\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1629\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1630\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1631\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m   1478\u001b[0m                                     \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m                                     \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1480\u001b[0;31m                                     exception_prefix='target')\n\u001b[0m\u001b[1;32m   1481\u001b[0m         sample_weights = _standardize_sample_weights(sample_weight,\n\u001b[1;32m   1482\u001b[0m                                                      self._feed_output_names)\n",
      "\u001b[0;32m~/anaconda3/envs/pyannote/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                         \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected threshold_activation to have 3 dimensions, but got array with shape (645, 1)"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model.compile\n",
    "\n",
    "how_many_step = 2\n",
    "ix_step = 0\n",
    "from_epi = 0\n",
    "\n",
    "while (ix_step < how_many_step):\n",
    "    ix_step += 1\n",
    "    \n",
    "    input_array, output_array = create_data_for_supervised (\"./amicorpus/*/audio/\", 16, 32, 0, 2, True)\n",
    "    \n",
    "    max_len = 320 # how many frame will be taken\n",
    "    step = 240 # step size.\n",
    "\n",
    "    input_array_specified = []\n",
    "    output_array_specified = []\n",
    "\n",
    "    for i in range (0, input_array.shape[1]-max_len, step):\n",
    "        single_input_specified = np.transpose(input_array[:,i:i+max_len])\n",
    "        single_output_specified = np.transpose(output_array[:,i+max_len])\n",
    "        \n",
    "        input_array_specified.append(single_input_specified)\n",
    "        output_array_specified.append(single_output_specified)\n",
    "\n",
    "    output_array_specified = np.asarray(output_array_specified)\n",
    "    input_array_specified = np.asarray(input_array_specified)\n",
    "    \n",
    "    model.fit(input_array_specified, output_array_specified,\n",
    "               epochs=2,\n",
    "               batch_size=16,\n",
    "               shuffle=True)\n",
    "    \n",
    "    # https://keras.io/getting-started/faq/#how-can-i-save-a-keras-model\n",
    "\n",
    "    model.save_weights('bilstm_weights.h5')    \n",
    "    \n",
    "    input_array = []\n",
    "    output_array = []\n",
    "    \n",
    "    from_epi += 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
